{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# M2 | Exploration Notebook\n",
    "\n",
    "In this notebook, you will do a first exploration of the data set that you will use for your project. One part of this exploration is guided, i.e. we will ask you to solve specific questions (task 1-3). The other part is open, i.e. we will ask you to come up with your own exploration ideas (task 4). \n",
    "\n",
    "Please upload your solved notebook to Moodle (under Milestone 2 Submission)adding your SCIPER number in title, example: m2-lernnavi-456392.ipynb\n",
    "\n",
    "\n",
    "## Brief overview of Lernnavi\n",
    "[Lernnavi](https://www.lernnavi.ch) is an instrument for promoting part of the basic technical study skills in German and mathematics.\n",
    "\n",
    "\n",
    "For the guided part of the exploration we will focus on the three main tables:\n",
    "* *users*: demographic information of users.\n",
    "* *events*: events done by the users in the platform.\n",
    "* *transactions*: question and answer solved by user.\n",
    "\n",
    "### Users\n",
    "* user_id: unique identifier of user in database.\n",
    "* gender: only three values: M male, F female or missing (star). \n",
    "* canton: swiss canton.\n",
    "* class_level: school year in swiss system.\n",
    "* study: boolean variable. True if the student participated in the study.\n",
    "* class_id: identifier of student’s class (only for the students in the experiment)\n",
    "\n",
    "\n",
    "### Events\n",
    "* event_id: unique identifier of event in database.\n",
    "* user_id: user who peformed the event.\n",
    "* event_date: timestamp of event.\n",
    "* category: classification of action (task, general, statistics, etc).\n",
    "* action: type of action performed.\n",
    "* event_type: whether the students viewed or clicked in the event.\n",
    "* transaction_token: used to link to transactions table.\n",
    "* tracking_data: optional content associated to this event (e.g., the new points mastered for a topic).\n",
    "* session_id: session during which the event took place.\n",
    "* topic_id: the topics represent the taxonomy of categories shown in the Deutsch and Math dashboard. See topics_translated table.\n",
    "* session_closed: whether the session has been finished (1: finished; 0: not finished).\n",
    "* session_type: whether the session is a learn or level check (1: learn; 2: level check).\n",
    "* session_accepted: whether the user finally accepted the result of the session (1: accepted; 0: refused).\n",
    "\n",
    "### Transactions\n",
    "* transaction_id: unique identifier of transaction in database.\n",
    "* transaction_token: used to link to events table.\n",
    "* user_id: user who performed the transaction.\n",
    "* document_id: document that was answered in transaction.\n",
    "* document_version: version of document that was answered.\n",
    "* evaluation: whether the user answered correctly or not. It is possible that it was only partially right. \n",
    "* input: answer the user gave.\n",
    "* start_time: timestamp of when the user started answering.\n",
    "* commit_time: timestamp of when the user submitted the answer.\n",
    "* user_agent: the browser that the user used.\n",
    "* solution: solution to question.\n",
    "* type: type of question that was answered. \n",
    "* session_id: session during which the event took place.\n",
    "* topic_id: the topics represent the taxonomy of categories shown in the Deutsch and Math dashboard. See topics_translated table.\n",
    "* session_closed: whether the session has been finished (1: finished; 0: not finished).\n",
    "* session_type: whether the session is a learn or level check (1: learn; 2: level check).\n",
    "* session_accepted: whether the user finally accepted the result of the session (1: accepted; 0: refused).\n",
    "* challenge: (boolean) whether the transaction was part of a challenge or not. Professors can create challenges containing different documents\n",
    "* challenge_id: unique identifier of challenges. The same challenge can be done by multiple students. The pre-test and post-test in the study were designed like challenges.\n",
    "* challenge_order: within the challenge, the order of the questions. The order matters because sometimes the questions were adapted depending on the student’s knowledge.\n",
    "* challenge_name: name given to the challenges. \n",
    "\n",
    "## Useful Metadata Files\n",
    "* [Data description](https://docs.google.com/document/d/1NPFNwi79JddrxZM-CpltH5nHro5btHRSNnYcAGj7Y0A/edit?usp=sharing)\n"
   ],
   "id": "c32aca56c9acb8e6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Import the tables of the data set as dataframes.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DATA_DIR = '../data' #You many change the directory\n",
    "\n",
    "\n",
    "users = pd.read_csv('{}/users.csv.gz'.format(DATA_DIR))\n",
    "events = pd.read_csv('{}/events.csv.gz'.format(DATA_DIR))\n",
    "transactions = pd.read_csv('{}/transactions.csv.gz'.format(DATA_DIR))"
   ],
   "id": "f2b5cbdb0ce37aa7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Task 1: Simple Statistics\n",
    "\n",
    "In this task you are asked to do a first coarse exploration of the data set, using simple statistics and visualizations."
   ],
   "id": "f28cd1f8b4369eba"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### a) How many distinct participants do we have in the transactions table?\n",
   "id": "ec945da022fe1f84"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# We count the number of unique user_id in the transactions table\n",
    "nb_user_in_trans = len(transactions['user_id'].unique())\n",
    "print(nb_user_in_trans)"
   ],
   "id": "f902ec55ef05b12b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### b) How many transactions did each user do? Please provide a visualization and discuss the distribution.",
   "id": "a2929bd75a527c18"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# We group by the user id to count the size of each group.\n",
    "nb_trans_p_user = transactions.groupby('user_id').size()\n",
    "\n",
    "# We plot it using a bar plot.\n",
    "f = plt.figure()\n",
    "plt.bar(nb_trans_p_user.keys(), nb_trans_p_user, log=True, width=50)\n",
    "f.set_figwidth(15)\n",
    "plt.title('Bar plot of the number of transactions per user id')\n",
    "plt.xlabel('User id')\n",
    "plt.ylabel('Number of transactions')\n",
    "plt.show()"
   ],
   "id": "3f72a59f3aa98270"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Discussion\n",
    "We see that the ids are not really well distributed, some user ids did not do any transaction. It may be because people created an account because of an order from the school or to test but never had the necessity to use it."
   ],
   "id": "89ba9ee81067976d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### c) Which are the 8 most popular event actions? Please provide a visualization of the frequency of the top 8 event actions.\n",
    "\n",
    "Hint: See actions in table events."
   ],
   "id": "abb71743f523fb43"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# We group the events table by action, count the number of elements for each group, sort the group and only take the 8 first elements.\n",
    "eight_most_pop = events.groupby(['action']).size().sort_values(ascending=False)[:8]\n",
    "\n",
    "print('The eight more popular actions:')\n",
    "print(eight_most_pop)\n",
    "\n",
    "# Let's set a great plot size\n",
    "f = plt.figure()\n",
    "f.set_figwidth(16)\n",
    "f.set_figheight(4)\n",
    "\n",
    "# We plot the bar plot\n",
    "plt.bar(eight_most_pop.keys(), eight_most_pop)\n",
    "plt.title('Bar plot of the eight most popular actions')\n",
    "plt.xlabel('Action taken')\n",
    "plt.ylabel('Number of event of this action')\n",
    "plt.show()\n",
    "\n",
    "# Let's do a pie chart to see the distribution (Warning! It does not take into account the actions other than the eight first!)\n",
    "plt.pie(eight_most_pop, labels=eight_most_pop.keys())\n",
    "plt.title('Pie plot of the eight most popular actions')\n",
    "plt.show()\n"
   ],
   "id": "5efccfa4f6485776"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Discussion\n",
    "\n",
    "The fact the PAGE_VISIT is the most popular action is not a surprise. People are often going back and forth through pages and URL changes happen quite often.\n",
    "REVIEW_TASK and SUBMIT_ANSWER are the second and third most popular actions, which make sense since people are often reviewing and submitting questionnaires."
   ],
   "id": "d946c9a067aafd02"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Task 2: Static Analysis\n",
    "\n",
    "In this second task, you will do a univariate an multivariate exploration of some aggregated features."
   ],
   "id": "7ec96607da1106b7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### a) Build a data frame containing one row per user:\n",
    "\n",
    "``[user_id, gender, num_pages_visited, num_feedback, percentage_correct, num_clicks]``\n",
    "\n",
    "\n",
    "The features are defined as follows:\n",
    "\n",
    "- **num_pages_visited**: total number of pages a student visited \n",
    "\n",
    "- **num_feedback**: total number of times the student opened the feedback\n",
    "\n",
    "- **percentage_correct**: number of correct answers/total number of answers (hint: evaluation = correct). If desired, you may assign a weight of 0.5 to partially correct answers. \n",
    "\n",
    "- **num_clicks**: total number of click events (hint: event_type = CLICK)"
   ],
   "id": "db955eed0bb82983"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Let's first see what is the name of all the actions\n",
    "print(events['action'].unique())"
   ],
   "id": "65842aa92985d932"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# We only take the 'PAGE_VISIT' actions, group the DataFrame by user id and count the number of occurrences. (Warning! We count the number of pages visited, not the distinct number of pages!)\n",
    "page_visited_per_user = events[events['action'] == 'PAGE_VISIT'][['user_id', 'action']].groupby('user_id').size().rename('num_pages_visited')\n",
    "page_visited_per_user.head()"
   ],
   "id": "81729dc5625eb467"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# We do the same with the 'OPEN_FEEDBACK' action\n",
    "feedback_per_user = events[events['action'] == 'OPEN_FEEDBACK'][['user_id', 'action']].groupby('user_id').size().rename('num_feedback')\n",
    "feedback_per_user.head()"
   ],
   "id": "7801b2b023a58a0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# To compute the percentage, we first drop the nans\n",
    "corrected_df = transactions[['user_id', 'evaluation']].dropna(how='any')\n",
    "\n",
    "# We compute the number of correct answers and all the numbers of answers for each user id\n",
    "nb_correct = corrected_df[corrected_df['evaluation'] == 'CORRECT'].groupby(['user_id']).size().rename('correct')\n",
    "nb_partial = corrected_df[corrected_df['evaluation'] == 'PARTIAL'].groupby(['user_id']).size().rename('partial')\n",
    "nb_all = corrected_df.groupby('user_id').size().rename('all')\n",
    "\n",
    "# Let's join and compute the ratio\n",
    "percentage_correct = pd.merge(nb_correct, nb_partial, on='user_id', how='outer').join(nb_all, on='user_id', how='outer').set_index('user_id').fillna(0)\n",
    "percentage_correct['percentage_correct'] = ((percentage_correct['correct']+0.5*percentage_correct['partial'])/percentage_correct['all'])\n",
    "\n",
    "# We drop the old tables to free memory\n",
    "del nb_correct\n",
    "del nb_partial\n",
    "del nb_all\n",
    "del corrected_df\n",
    "\n",
    "# We drop the useless columns\n",
    "percentage_correct.drop('correct', axis=1, inplace=True)\n",
    "percentage_correct.drop('partial', axis=1, inplace=True)\n",
    "percentage_correct.drop('all', axis=1, inplace=True)\n",
    "\n",
    "percentage_correct.head()"
   ],
   "id": "3a1bcdb65443c987"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# We do the same than PAGE_VISITED but for the CLICK event type\n",
    "nb_clicks = events[events['event_type'] == 'CLICK'][['user_id', 'action']].groupby('user_id').size().rename('num_clicks')\n",
    "nb_clicks.head()"
   ],
   "id": "6786eb3bcf9805f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# We combine all the dataframes to one\n",
    "combined_df = users[['user_id', 'gender']].set_index('user_id').join(page_visited_per_user, on='user_id', how='outer').join(feedback_per_user, on='user_id', how='outer').join(percentage_correct, on='user_id', how='outer').join(nb_clicks, on='user_id', how='outer')\n",
    "\n",
    "# Let's consider all Nan values of counts to 0 (since it is Nan because no entry was found) and cast the counts to int.\n",
    "combined_df[['num_pages_visited', 'num_feedback', 'num_clicks']] = combined_df[['num_pages_visited', 'num_feedback', 'num_clicks']].fillna(0)\n",
    "combined_df[['num_pages_visited', 'num_feedback', 'num_clicks']] = combined_df[['num_pages_visited', 'num_feedback', 'num_clicks']].astype(int)\n",
    "\n",
    "# Free memory\n",
    "del page_visited_per_user, feedback_per_user, percentage_correct, nb_clicks\n",
    "\n",
    "combined_df.head()"
   ],
   "id": "cb4e3fc695895d9a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "b) Perform a univariate analysis (including descriptive statistics and visualizations) for the five features (gender, num_pages_visited, num_feedback, percentage_correct, num_clicks) of your dataframe. Please check the lecture slides regarding information on how to perform a univariate analysis for categorical and numerical features. Discuss your results: how are the features distributed? Are there any anomalities?",
   "id": "657bb081be18dd94"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# We can describe the form of the numerical values\n",
    "combined_df.describe()"
   ],
   "id": "e8a5c8fa5979f131"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Let's see all the distributions (apart of the genders for now)\n",
    "fig, ax = plt.subplots(2, 2)\n",
    "\n",
    "fig.set_figwidth(8)\n",
    "fig.set_figheight(6)\n",
    "\n",
    "ax[0,0].hist(combined_df['num_pages_visited'], bins=100, log=True)\n",
    "ax[0,0].set_title('Histogram of number of pages visited')\n",
    "\n",
    "ax[0,1].hist(combined_df['num_feedback'], bins=100, log=True)\n",
    "ax[0,1].set_title('Histogram of number of feedbacks')\n",
    "\n",
    "ax[1,0].hist(combined_df['num_clicks'], bins=100, log=True)\n",
    "ax[1,0].set_title('Histogram of number of clicks')\n",
    "\n",
    "ax[1,1].hist(combined_df['percentage_correct'], bins=50)\n",
    "ax[1,1].set_title('Histogram correct percentages')\n",
    "plt.show()"
   ],
   "id": "c8a9f029193ea41b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# A boxplot to see the percentage distribution better (Since the other variables follow a power law, a box plot is not really interesting)\n",
    "plt.boxplot(combined_df['percentage_correct'].dropna())\n",
    "plt.title('Box plot of the percentage correct')\n",
    "plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "id": "c71ad4677479f7df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# For the categorical values, we need to add more steps (We change the Nans to 'NoData' since it can be interesting to see how much there are)\n",
    "genders = combined_df['gender'].fillna('NoData').value_counts()\n",
    "genders"
   ],
   "id": "14a2405bc7fec14f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Let's see a pie chart for the distribution on the genders\n",
    "plt.pie(genders, labels=genders.keys(), autopct='%1.1f%%')\n",
    "plt.title('Gender distribution')\n",
    "plt.show()\n",
    "\n",
    "# If we plot only the known ones\n",
    "only_known = genders.loc[['MALE', 'FEMALE']]\n",
    "plt.pie(only_known, labels=only_known.keys(), autopct='%1.1f%%')\n",
    "plt.title('Known gender distribution')\n",
    "plt.show()\n",
    "\n",
    "#Free memory\n",
    "del genders\n",
    "del only_known"
   ],
   "id": "81df29e9650eea49"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Discussion\n",
    "\n",
    "From this, we can do some interesting observations:\n",
    "1. The counts (number of page visited, feedbacks and clicks) follow an exponential distribution. There are a lot of 0 but some of the users have some really high amounts.\n",
    "2. The percentage of correct answers shows that the mean and median is around 66%. People are then correct two third of the time.\n",
    "3. The genders are not uniformly distributed. There are sensibly more women than men. The explication is not clear but it may be because young women tend to be more serious and motivated than young men. Also, while there a little amount of non given genders (STAR), there are a big amount of not defined gender which can be a problem for data analysis."
   ],
   "id": "f679f2e36025c85f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "c) Come up with two additional features on your own and add them to the dataframe. Please provide an explanation/description of your features as well as an argument/hypothesis of why you think these features are interesting to explore.",
   "id": "47d33efd70aac0f0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "First, it would be really interesting to extract the operating system used.",
   "id": "b3186e9fd9cb1e63"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# We first declare the function to split, get the OS and remove punctuation.\n",
    "import string\n",
    "def strip_os(row):\n",
    "    # For each row, we take the correct column that we split.\n",
    "    list_str = str(row['user_agent']).split()\n",
    "\n",
    "    # If the len is >= 2, we take the second term which correspond to the OS. If there are no more than 1 element, it mean that is is either Nan or unknown\n",
    "    if len(list_str) >= 2:\n",
    "        ret_string = list_str[1]\n",
    "    else:\n",
    "        ret_string = list_str[0]\n",
    "\n",
    "    # We remove punctuation\n",
    "    return ''.join(filter(lambda char: char not in string.punctuation, ret_string))\n",
    "\n",
    "# We take the wanted columns\n",
    "agent_df = transactions[['user_id', 'user_agent']].copy()\n",
    "\n",
    "# We watch how is this column\n",
    "print(agent_df.head())\n",
    "\n",
    "# We apply the function\n",
    "agent_df['user_agent'] = agent_df.apply(strip_os, axis=1)"
   ],
   "id": "e8034cbec937d851"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# To get the favorite OS, we need to get rid of 'nan' and 'unknown'\n",
    "agent_df = agent_df[(agent_df['user_agent'] != 'nan') & (agent_df['user_agent'] != 'unknown')].copy()\n",
    "# We then count the number of time the user used any OS\n",
    "agent_df = agent_df.groupby(['user_id', 'user_agent']).size().rename('count')"
   ],
   "id": "49afc901c6d8b86"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# We take the max\n",
    "agent_df = agent_df.reset_index().groupby(['user_id']).max()['user_agent']\n",
    "agent_df.head()"
   ],
   "id": "96aacd45d455a2da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# We are doing a left join because having a OS without anything else is not relevant\n",
    "combined_df = combined_df.join(agent_df, on='user_id', how='left')"
   ],
   "id": "4f4d616fc985c30c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Secondly, we could extract the canton of the user.",
   "id": "5d270151a9682d53"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "cantons_df = users[['user_id', 'canton']].set_index('user_id')",
   "id": "8d8943281dfca913"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "cantons_df.head()",
   "id": "baa8baa4302495ad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# We are doing a left join because having a canton without anything else is not relevant\n",
    "combined_df = combined_df.join(cantons_df, on='user_id', how='left')\n",
    "\n",
    "# Free memory\n",
    "del cantons_df"
   ],
   "id": "6727db5686cf8257"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# We verify that all is OK\n",
    "combined_df.head()"
   ],
   "id": "95a0d4baa8c52aa9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Discussion\n",
    "#### 1\n",
    "The choice of the first feature can be explained by the fact that some believes make differences between the users of certain OSs. Apple OSs are generally seen to be very user friendly and costly. Linux is on the contrary very cheap but hard to use and not very user friendly. Windows is seen between those two. An interesting hypothesis would be that since Apple OSs are more user friendly, these users may have better grades in german (Since the programs provided gives more advantages to do good texts in german) but may have lower grades in mathematics (Since 'Geeks' tends to love maths and linux).\n",
    "\n",
    "The feature 'user_agent' of the 'transactions' table gives the browser used by the user to do the transaction. Since the browser must be adapted for the OS, we can extract it.\n",
    "\n",
    "#### 2\n",
    "The second feature is straightforward but it could be very interesting to see the distribution of the students cantons. One could argue that swiss germans living far away from Germany (like Wallis) may have lower grades that those who are close to Germany (like Shaffausen). That is a great opportunity to verify that hypothesis!\n",
    "\n",
    "The 'canton' feature is just composed of the acronym of the canton of the user."
   ],
   "id": "c9ec34c0a675c890"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "d) Perform a univariate analysis of your features (including descriptive statistics and visualization). What can you observe? Do the results confirm your hypotheses?",
   "id": "403f4da047ca2711"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# For the OS feature\n",
    "os_counts = combined_df['user_agent'].value_counts()\n",
    "os_counts"
   ],
   "id": "8d0aab341c877f8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Let's see a pie chart for the distribution on the OSs\n",
    "plt.pie(os_counts, labels=os_counts.keys(), autopct='%1.1f%%')\n",
    "plt.title('OS distribution')\n",
    "plt.show()"
   ],
   "id": "12f44f83030e70ef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "cantons_count = combined_df['canton'].value_counts()\n",
    "cantons_count"
   ],
   "id": "dd8a335fbfcb1b53"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Let's see a pie chart for the distribution on the OSs\n",
    "\n",
    "plt.pie(cantons_count, labels=cantons_count.keys(), autopct='%1.1f%%')\n",
    "plt.title('Cantons distribution')\n",
    "plt.show()"
   ],
   "id": "914e5624fc51161b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Discussion\n",
    "#### 1\n",
    "We can see some very interesting facts! The big majority is using Windows as OS. Apple is coming second with the OSs like Macintosh, Iphone OS and Ipad OS. Open source software are really not often used, since Linux, X11 and Android (semi-open...) seem to be pouted by students. It make sense since open source OSs are more of a computer scientists thing.\n",
    "\n",
    "#### 2\n",
    "There are no real surprises for the values of the cantons. Zurich have the most students which is normal since it is the most populated canton. Romands cantons do not use lernnavi, they are so almost excluded from the data. (Some are still in but it maybe because people may be bilingual and motivated or they do some tests to understand the site to do analysis on the data (-: )\n",
    "\n",
    "The only not obvious thing is that Bern has a weirdly low amount of users. Since it is a swiss german the second most populated canton, that is a surprise. Maybe the canton just does not really want to use lernnavi or use another site."
   ],
   "id": "b3d00a177fd66966"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "e) Perform a multivariate analysis for two pairs of features of your choice. Please provide a metric and a visualization for both pairs. Please discuss: why did you choose these two pairs? What was your hypothesis? Do the results confirm your hypothesis?",
   "id": "f37c4375bfd53c20"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We said earlier that maybe the people with open source OSs may have better grades in mathematics and people with Apple OSs may have better grades in german. Let's first see if there is a global influence of the OS on the grades.",
   "id": "9acbe8c2ddd425a4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# We can use the same hypothesis we saw before\n",
    "percent_agent = combined_df[['percentage_correct', 'user_agent']].groupby(['user_agent']).mean().reset_index()\n",
    "percent_agent.head()"
   ],
   "id": "e9e0da09c8744c5f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.bar(x=percent_agent['user_agent'], height=percent_agent['percentage_correct'])\n",
    "plt.title('Mean correctness percentage per OS')\n",
    "plt.show()"
   ],
   "id": "48469db06350b63e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Then, we can do the same for the canton! Are there cantons with higher grade?",
   "id": "403f9ca471c462fa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# We can use the same hypothesis we saw before\n",
    "percent_canton = combined_df[['percentage_correct', 'canton']].groupby(['canton']).mean().reset_index()\n",
    "percent_canton"
   ],
   "id": "42ecbf0a03711677"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.bar(x=percent_canton['canton'], height=percent_canton['percentage_correct'])\n",
    "plt.title('Mean correctness percentage per canton')\n",
    "plt.show()"
   ],
   "id": "f6f8dd7d93be6880"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Since the last two multivariate analysis were a little bit disappointing and only involved one numeric and one categorical values, let's do a third.\n",
    "\n",
    "If somebody clicks on a lot of things, it is likely that he/she visits a lot of pages. Let verify that!"
   ],
   "id": "9e034ebb861fb4d0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "clicks_pages = combined_df[['num_clicks', 'num_pages_visited']]",
   "id": "b30a47c982d25df3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Let's do a scatter plot to see what is going on\n",
    "plt.scatter(clicks_pages['num_clicks'], clicks_pages['num_pages_visited'])\n",
    "plt.xlabel('Number of clicks')\n",
    "plt.ylabel('Number of pages visited')\n",
    "plt.title('Clicks and pages visited distribution')\n",
    "plt.show()"
   ],
   "id": "373d1f294be441ef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# We compute the Pearson correlation\n",
    "clicks_pages.corr()"
   ],
   "id": "768a260b15f92483"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Free memory\n",
    "del clicks_pages, percent_canton, percent_agent"
   ],
   "id": "f56e6c422e3eb7ed"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Discussion\n",
    "#### 1\n",
    "The first analysis is not really interesting... We do not see any notable values in the graph and the only one that may differ (Android) is not very representative since it is the preferred OS of only 8 people... We cannot conclude anything with this.\n",
    "\n",
    "#### 2\n",
    "The second analysis is not better... We can see that the Romand cantons does not have any percentages (Nan). It may confirm the hypothesis that states that Romand cantons only did tests. Apart from that, we cannot conclude anything on that.\n",
    "\n",
    "#### 3\n",
    "The third analysis is interesting, although obvious. In the scatter plot, we see a clear tendency of a positive linear correlation, which is approved by the high Pearson correlation score. We can say that the hypothesis is confirmed."
   ],
   "id": "59724ab6b5f8429f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Task 3: Time-Series Analysis\n",
    "\n",
    "In the last task, you will perform a time-series analysis."
   ],
   "id": "e217baf39bed378a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "#### a) Build a data frame containing one row per user per week:\n",
    "\n",
    "``[user_id, week, num_pages_visited, num_feedback, percentage_correct, num_clicks]``\n",
    "\n",
    "\n",
    "The features are defined as follows:\n",
    "\n",
    "- **num_pages_visited**: total number of pages a student visited \n",
    "\n",
    "- **num_feedback**: total number of times the student opened the feedback\n",
    "\n",
    "- **percentage_correct**: number of correct answers/total number of answers (hint: evaluation = correct). If desired, you may assign a weight of 0.5 to partially correct answers. \n",
    "\n",
    "- **num_clicks**: total number of click events (hint: event_type = CLICK)\n",
    "\n",
    "Where week 0 is the first week the specific user solved a task in the platform, i.e., the user's earliest entry in the transactions table.\n",
    "\n",
    "Hint: You may extract the week of the year (dt.week) from the timestamps.\n",
    "\n",
    "Hint 2: Be mindful that week 1 in 2022 is a different week from week 1 in 2023."
   ],
   "id": "46c3635fff928833"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "You can limit the number of weeks to 10, i.e. for each user we just look at the first 10 weeks of data.\n",
    "You may change and justify your choice for the number of weeks."
   ],
   "id": "e61b7f8f4030bfd0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Explications\n",
    "1. I decided to keep all weeks in the DataFrames since it could have been actions taken before the first transaction of a user and some behaviors can be very interesting to see at the very end or very beginning of the user's usage of the site.\n",
    "2. I also decided to consider weeks as just 7 days and not taking into account the specific day of the week. For example, if a user did the first commit on a sunday, the next monday will still be in the 0th week and not the first one since only one day has passed. The first week will begin the next sunday, when 7 days will have passed."
   ],
   "id": "eff2acdfdadd7509"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "transactions.head()",
   "id": "8ccf1dbd30ad17c0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We first need to compute the first commit time of each user.",
   "id": "ca18ab303c37a535"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# We first need to compute the earliest date where each user solved a task\n",
    "earliest_commit_time = transactions[['user_id', 'start_time']].copy()\n",
    "earliest_commit_time['start_time'] = pd.to_datetime(earliest_commit_time['start_time'])\n",
    "earliest_commit_time = earliest_commit_time.groupby('user_id').min().rename(columns = {'start_time': 'earliest_commit'})\n",
    "earliest_commit_time.head()"
   ],
   "id": "e2d1a8a3279510ed"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As the computation of the three 'num_*' columns are very similar, we can write a function.",
   "id": "174bd64bed6f37e1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def extract_week_from_event_action(action_type: str, column: str) -> pd.Series:\n",
    "    # We do the same as in Task 2 but we take also the event date\n",
    "    extracted_df = events[events[column] == action_type][['user_id', 'event_date']].set_index('user_id')\n",
    "\n",
    "    # We convert the date string to a real date\n",
    "    extracted_df['event_date'] = pd.to_datetime(extracted_df['event_date'])\n",
    "\n",
    "    # We join the earliest commit time\n",
    "    extracted_df = extracted_df.join(earliest_commit_time, on='user_id', how='left')\n",
    "\n",
    "    # We compute the week\n",
    "    extracted_df['week'] = (extracted_df['event_date'] - extracted_df['earliest_commit'])/np.timedelta64(1, 'W')\n",
    "\n",
    "    # We can drop the useless columns\n",
    "    extracted_df.drop('event_date', axis=1, inplace=True)\n",
    "    extracted_df.drop('earliest_commit', axis=1, inplace=True)\n",
    "\n",
    "    # Let's drop the nans (Otherwise, we cannot set week as index\n",
    "    extracted_df.dropna(subset=['week'], inplace=True)\n",
    "\n",
    "    # We round the week to the lower integer (since Day + 6 is still in the 0th week)\n",
    "    extracted_df = extracted_df['week'].astype(int)\n",
    "\n",
    "    # We group, count and return\n",
    "    return extracted_df.reset_index().groupby(['user_id', 'week']).size().rename('num')"
   ],
   "id": "2aff41e3dd95bd1f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "page_visited_per_user = extract_week_from_event_action('PAGE_VISIT', 'action').rename('num_pages_visited')\n",
    "page_visited_per_user.head()"
   ],
   "id": "3f0236306ba7ff13"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "feedback_per_user = extract_week_from_event_action('OPEN_FEEDBACK', 'action').rename('num_feedback')\n",
    "feedback_per_user.head()"
   ],
   "id": "3658f8846e1e952"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "num_clicks = extract_week_from_event_action('CLICK', 'event_type').rename('num_clicks')\n",
    "num_clicks.head()"
   ],
   "id": "d39bbae2de512bf2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The percentage is a little bit more tricky, since we have to compute the ratio in between",
   "id": "8a4887e150600e1a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Same as task 2 but with 'commit_time' (We take 'commit_time' as it is the time during which the answer was submitted)\n",
    "percentage_correct = transactions[['user_id', 'evaluation', 'commit_time']].dropna(how='any')\n",
    "\n",
    "# We convert the date string to a real date\n",
    "percentage_correct['commit_time'] = pd.to_datetime(percentage_correct['commit_time'])\n",
    "\n",
    "# We join the earliest commit time\n",
    "percentage_correct = percentage_correct.join(earliest_commit_time, on='user_id', how='left')\n",
    "\n",
    "# We compute the week\n",
    "percentage_correct['week'] = (percentage_correct['commit_time'] - percentage_correct['earliest_commit'])/np.timedelta64(1, 'W')\n",
    "\n",
    "# We can drop the useless columns\n",
    "percentage_correct.drop('commit_time', axis=1, inplace=True)\n",
    "percentage_correct.drop('earliest_commit', axis=1, inplace=True)\n",
    "\n",
    "# We round the week to the lower integer (since Day + 6 is still in the 0th week)\n",
    "percentage_correct['week'] = percentage_correct['week'].astype(int)\n",
    "\n",
    "# We compute the number of correct answers and all the numbers of answers for each user id\n",
    "nb_correct = percentage_correct[percentage_correct['evaluation'] == 'CORRECT'].groupby(['user_id', 'week']).size().rename('correct').astype(int)\n",
    "nb_partial = percentage_correct[percentage_correct['evaluation'] == 'PARTIAL'].groupby(['user_id', 'week']).size().rename('partial').astype(int)\n",
    "nb_all = percentage_correct.groupby(['user_id', 'week']).size().rename('all').astype(int)\n",
    "\n",
    "# Let's join and compute the ratio\n",
    "nb_df = pd.merge(nb_correct, nb_partial, on=['user_id', 'week'], how='outer').join(nb_all, on=['user_id', 'week'], how='outer').fillna(0)\n",
    "nb_df['percentage_correct'] = ((nb_df['correct']+0.5*nb_df['partial'])/nb_df['all'])\n",
    "\n",
    "# We drop the old tables to free memory\n",
    "del nb_correct\n",
    "del nb_partial\n",
    "del nb_all\n",
    "\n",
    "# We drop the useless columns\n",
    "nb_df.drop('correct', axis=1, inplace=True)\n",
    "nb_df.drop('partial', axis=1, inplace=True)\n",
    "nb_df.drop('all', axis=1, inplace=True)\n",
    "percentage_correct.drop('evaluation', axis=1, inplace=True)\n",
    "\n",
    "# We then join the ratio to the base DataFrame\n",
    "percentage_correct = percentage_correct.join(nb_df, on=['user_id', 'week'], how='outer')\n",
    "\n",
    "# Let's drop the nans (Otherwise, we cannot set week as index and it propagates during the mean() of the evaluations)\n",
    "percentage_correct = percentage_correct.dropna()\n",
    "\n",
    "# We group by and count\n",
    "percentage_correct = percentage_correct.groupby(['user_id', 'week']).mean()\n",
    "percentage_correct.head()"
   ],
   "id": "6acbbdf050f5f22c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Now, we can combine the DataFrames\n",
    "combined_week_df = pd.merge(page_visited_per_user, feedback_per_user, on=['user_id', 'week'], how='outer').join(percentage_correct, on=['user_id', 'week'], how='outer').join(num_clicks, on=['user_id', 'week'], how='outer')\n",
    "\n",
    "# Let's consider all Nan values of counts to 0 (since it is Nan because no entry was found) and cast the counts to int.\n",
    "combined_week_df[['num_pages_visited', 'num_feedback', 'num_clicks']] = combined_week_df[['num_pages_visited', 'num_feedback', 'num_clicks']].fillna(0)\n",
    "combined_week_df[['num_pages_visited', 'num_feedback', 'num_clicks']] = combined_week_df[['num_pages_visited', 'num_feedback', 'num_clicks']].astype(int)\n",
    "\n",
    "combined_week_df.head()"
   ],
   "id": "9578a460f0df8ea3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Free memory\n",
    "del page_visited_per_user, feedback_per_user, percentage_correct, num_clicks"
   ],
   "id": "bfaa6a85e4a07b58"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for user_id, group in combined_week_df.reset_index().groupby('user_id'):\n",
    "    #print(group)\n",
    "    plt.plot(group['week'], group['num_feedback'])\n",
    "    plt.show()"
   ],
   "id": "a797a72f90dfea70"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### b) Select two features and analyze their behavior over time. Please provide a hypothesis and visualization for both features. For ideas on how to perform a time series exploration, please check the lecture slides and notebook. Discuss your results: what do you observe? Do the results confirm your hypotheses?",
   "id": "551be6cc47f045b2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Hypothesis\n",
    "The two features chosen are 'num_pages_visited' and 'percentage_correct'.\n",
    "1. 'num_pages_visited': It would make sense that the motivation of the student are dropping through the year so the number of pages visited should decrease over time.\n",
    "2. 'percentage_correct': Even if the motivation is decreasing, the students must become better and better on the topic they see so the grades must increase over time."
   ],
   "id": "7d435b5155635996"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Let's describe the number of pages visited\n",
    "combined_week_df.reset_index().groupby('week')['num_pages_visited'].sum().describe()"
   ],
   "id": "c9ccdb7b5b194b39"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# And the percentage\n",
    "combined_week_df.reset_index().groupby('week')['percentage_correct'].mean().describe()"
   ],
   "id": "aef0d356512174bd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# The three 'num_*' columns are very similar, so let's just plot the three\n",
    "for column_name in ['num_pages_visited', 'num_feedback', 'num_clicks']:\n",
    "    sum_pages_per_week = combined_week_df.reset_index()[['week', column_name]].groupby('week').sum()\n",
    "    plt.bar(sum_pages_per_week.index, sum_pages_per_week[column_name], log=True)\n",
    "    refactored_name = column_name.replace('_', ' ').replace('num', 'Number of')\n",
    "    plt.title(f\"{refactored_name} through weeks\")\n",
    "    plt.xlabel('Week')\n",
    "    plt.ylabel(refactored_name)\n",
    "    plt.show()"
   ],
   "id": "20cdf1b402adfad1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Finally, let's plot the mean percentage over users.\n",
    "mean_percentage_per_week = combined_week_df.reset_index()[['week', 'percentage_correct']].groupby('week').mean()\n",
    "plt.bar(mean_percentage_per_week.index, mean_percentage_per_week['percentage_correct'])\n",
    "plt.title('Correct percentage through weeks')\n",
    "plt.xlabel('Week')\n",
    "plt.ylabel('Correct percentage')\n",
    "plt.show()"
   ],
   "id": "4fa9b83af71c298d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Discussion\n",
    "#### num_pages_visited\n",
    "The first interesting thing we can see is that the number of pages visited through weeks has a really high STD. It makes sense since we can expect some weeks to have a really low value (for example weeks before the first commit) and some have really high values (just after the first commit) (This may be a downgrade about keeping all weeks).\n",
    "On the plot, we see a gradual increase of the number of page visited before the first commit and then an explosion on the week of the first commit followed by a big drop afterward. This can be explained by the fact that before the first commit, only teachers or really motivated students go to test the site. Then the first commit is most likely mandatory which is why there is a big explosion of the number of commits the first week. Then the interest is most likely gradually decreasing through the weeks.\n",
    "\n",
    "(PS: We see that there can be clicks or page visits before the first commit, but no feedback. It makes sense since we can only do a feedback after having commited something)\n",
    "\n",
    "We can therefore partially accept the hypothesis (the number is increasing before but decreasing after)\n",
    "#### percentage_correct\n",
    "The describe method os not really interesting on the percentage_correct column. However, it is fascinating to analyse the plot. We see that the percentage correct is stable through the weeks, but have a little peak at the end. This could be explained by the fact that new topics are continuously taught so the grade are stable through the year but during the revisions, people are getting better and better everywhere. However, we have to keep in mind that some other variables can enter into account. For example, after the 80th week, only a few people are doing something on the site which could also explain the behavior of the plot.\n",
    "\n",
    "Therefore, I think we can refuse the hypothesis. (The percentage is not gradually increasing).\n"
   ],
   "id": "ab780408b4fe1b6e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Task 4: Creative extension \n",
    "\n",
    "Please provide **one** new hypothesis you would like to explore with the data and provide a visualization for it. Discuss your results: what do you observe? Do the results confirm your hypotheses?\n",
    "\n"
   ],
   "id": "568dccf99c211e33"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Hypothesis\n",
    "It would be interesting to see the evolution over time of grade of users of different OSs. It is quite difficult to make an hypothesis on that but we could suppose that Apple OSs users may keep a better grade through weeks than Linux users because the user-friendly applications are more convenient. Windows users should lie in between."
   ],
   "id": "244e3ff1dcaccdcf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# We need to first join the agent dataframe to our main temporal one\n",
    "combined_agent_week_df = combined_week_df.join(agent_df, on='user_id', how='left')\n",
    "combined_agent_week_df.head()"
   ],
   "id": "6ca0176d5e820f73"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Let's only take the wanted columns and compute the mean over the users by agent\n",
    "combined_agent_week_df = combined_agent_week_df.reset_index()[['week', 'user_agent', 'percentage_correct']].groupby(['user_agent', 'week']).mean()\n",
    "combined_agent_week_df = combined_agent_week_df.reset_index()"
   ],
   "id": "fd052997b274304b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from scipy.ndimage import gaussian_filter1d\n",
    "for agent in combined_agent_week_df['user_agent'].unique():\n",
    "\n",
    "    # We take the corresponding agent\n",
    "    current_df = combined_agent_week_df[combined_agent_week_df['user_agent'] == agent][['week', 'percentage_correct']]\n",
    "\n",
    "    # We smooth the graph, otherwise it is un readable\n",
    "    ysmoothed = gaussian_filter1d(current_df.percentage_correct, sigma=2)\n",
    "\n",
    "    # We plot it\n",
    "    plt.plot(current_df.week, ysmoothed, label=agent)\n",
    "\n",
    "plt.title('Correct percentage through weeks')\n",
    "plt.xlabel('Week')\n",
    "plt.ylabel('Correct percentage')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ],
   "id": "4fdb73696598ab42"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Discussion\n",
    "We can see from the graph multiple interesting fact:\n",
    "1. First, the Android and Linux graphs are absent, since there are not enough weeks with data in order to make a correct smoothing.\n",
    "2. There is no clear differences between Windows and Macintosh users...\n",
    "3. However, we see a big difference with the iPhone users. They have quite lower grades than the others and tend to gradually do worse through weeks. That could be explained that students doing their homework on their phones are not as motivated as the others.\n",
    "4. The iPad curve is very unstable. This is surely because their number is low.\n",
    "\n",
    "From these observations, we can deny our hypothesis since there are no real differences between Apple OSs and Windows users. However, students doing their homework on their phone tend to have lower grades than the others."
   ],
   "id": "f72f4c6977e33c02"
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "m2-lernnavi-sciper.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
